{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 第十一次作业"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 1 jieba分词\n",
    "\n",
    "jieba是一个Python中文分词组件，支持多种分词模式和自定义词典。\n",
    "\n",
    "安装jieba：``pip install jieba``\n",
    "\n",
    "参考网站：https://github.com/fxsjy/jieba"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 多种模式分词\n",
    "\n",
    "使用精确模式、全模式、搜索引擎模式对下面这段文本进行分词。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import jieba\n",
    "\n",
    "text = \"2021中国空间站“天和”核心舱已准备就绪，近日将在海南文昌航天发射场由长征五号B遥二运载火箭发射升空。在2021年~2022年间，中国将接续实施11次飞行任务，包括3次空间站舱段发射、4次货运飞船以及4次载人飞船发射，于2022年完成空间站在轨建造，实现中国载人航天工程“三步走”发展战略第三步的任务目标。\"\n",
    "\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Building prefix dict from the default dictionary ...\n",
      "Dumping model to file cache /tmp/jieba.cache\n",
      "Loading model cost 1.900 seconds.\n",
      "Prefix dict has been built successfully.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021 中国 空间 空间站 “ 天 和 ” 核心 舱 已 准备 准备就绪 就绪 ， 近日 将 在 海南 文昌 航天 发射 发射场 由 长征 五号 B 遥 二 运载 运载火箭 火箭 发射 升空 。 在 2021 年 ~ 2022 年间 ， 中国 将 接续 实施 11 次 飞行 任务 ， 包括 3 次 空间 空间站 舱 段 发射 、 4 次货 货运 运飞 飞船 以及 4 次 载人 飞船 发射 ， 于 2022 年 完成 空间 空间站 在 轨 建造 ， 实现 中国 载人 航天 天工 工程 “ 三步 三步走 ” 发展 战略 第三 第三步 三步 的 任务 目标 。\n"
     ]
    }
   ],
   "source": [
    "print(' '.join(jieba.cut(text,cut_all=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 自定义词典\n",
    "\n",
    "添加通过自定义词典或调整词典的方式，将上文中的“天和”，“长征五号”划分成一个词，输出精确模式的分词结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2021 中国 空间 空间站 “ 天和 ” 核心 舱 已 准备 准备就绪 就绪 ， 近日 将 在 海南 文昌 航天 发射 发射场 由 长征 长征五号 五号 B 遥 二 运载 运载火箭 火箭 发射 升空 。 在 2021 年 ~ 2022 年间 ， 中国 将 接续 实施 11 次 飞行 任务 ， 包括 3 次 空间 空间站 舱 段 发射 、 4 次货 货运 运飞 飞船 以及 4 次 载人 飞船 发射 ， 于 2022 年 完成 空间 空间站 在 轨 建造 ， 实现 中国 载人 航天 天工 工程 “ 三步 三步走 ” 发展 战略 第三 第三步 三步 的 任务 目标 。\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "jieba.add_word(\"天和\")\n",
    "jieba.add_word('长征五号')\n",
    "print(' '.join(jieba.cut(text,cut_all=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 关键词抽取\n",
    "\n",
    "使用jieba提供的TF-IDF和TextRank提取上面这段文本中的top 10个关键词。\n",
    "\n",
    "注：jieba在计算TF-IDF时会使用自带的idf文件和停用词表，在实际使用过程中可以根据需要切换成自定义语料库，这里用默认的即可。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['空间站', '2021', '2022', '发射', '载人', '飞船', '航天', '天和', '长征五号', '遥二']\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "import jieba.analyse\n",
    "kewords=jieba.analyse.extract_tags(text,topK=10)\n",
    "print(kewords)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2 Gensim\n",
    "\n",
    "Gensim是一款开源的第三方Python工具包，用于从原始的非结构化的文本中，无监督地学习到文本隐层的主题向量表达。\n",
    "\n",
    "安装Gensim：``pip install gensim``\n",
    "\n",
    "参考网站：https://radimrehurek.com/gensim/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 加载预训练模型\n",
    "\n",
    "gensim提供了一些语料和模型，可以通过数据接口下载。下载得到的模型默认保存在'~/gensim-data'目录下，详见https://github.com/RaRe-Technologies/gensim-data\n",
    "\n",
    "使用接口下载并载入名为\"glove-wiki-gigaword-50\"的预训练模型。\n",
    "\n",
    "注：KeyedVectors可以读取提前训练好的词向量。本次作业文件夹中的glove-wiki-gigaword-50.gz包含一个同名的txt文件，第一行为“400000 50”，表示存储了400000个词向量，每个词向量的维度为50，之后的每一行为一个词和它的词向量，例如“the 0.418 0.24968 -0.41242 0.1217 ...”。这样的词向量文件属于word2vec格式，用``load_word2vec_format()``方法载入模型。词向量文件存储时通常有word2vec和glove两种格式，区别在于glove格式没有第一行的向量数量和维度“400000 50”，具体可以浏览https://radimrehurek.com/gensim/scripts/glove2word2vec.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gensim\n",
    "from gensim.models import KeyedVectors\n",
    "import gensim.downloader as api\n",
    "\n",
    "# 下载预训练模型\n",
    "# model = api.load(\"glove-wiki-gigaword-50\") # download the model and return as object ready for use\n",
    "\n",
    "# 如果无法下载，运行下面的代码读取词向量\n",
    "model = KeyedVectors.load_word2vec_format('./glove-wiki-gigaword-50.gz')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请用载入的词向量模型完成以下几点：\n",
    "\n",
    "1. 查看'computer'的词向量\n",
    "\n",
    "2. 计算'computer'和'software'的相似程度\n",
    "\n",
    "3. 输出与'computer'最相似的十个词"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.079084 -0.81504   1.7901    0.91653   0.10797  -0.55628  -0.84427\n",
      " -1.4951    0.13418   0.63627   0.35146   0.25813  -0.55029   0.51056\n",
      "  0.37409   0.12092  -1.6166    0.83653   0.14202  -0.52348   0.73453\n",
      "  0.12207  -0.49079   0.32533   0.45306  -1.585    -0.63848  -1.0053\n",
      "  0.10454  -0.42984   3.181    -0.62187   0.16819  -1.0139    0.064058\n",
      "  0.57844  -0.4556    0.73783   0.37203  -0.57722   0.66441   0.055129\n",
      "  0.037891  1.3275    0.30991   0.50697   1.2357    0.1274   -0.11434\n",
      "  0.20709 ]\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "print(model['computer'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8814993\n"
     ]
    }
   ],
   "source": [
    "print(model.similarity('computer','software'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('computers', 0.9165045022964478),\n",
       " ('software', 0.8814992904663086),\n",
       " ('technology', 0.852556049823761),\n",
       " ('electronic', 0.812586784362793),\n",
       " ('internet', 0.8060455322265625),\n",
       " ('computing', 0.802603542804718),\n",
       " ('devices', 0.8016185760498047),\n",
       " ('digital', 0.7991793751716614),\n",
       " ('applications', 0.7912740707397461),\n",
       " ('pc', 0.7883159518241882)]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.most_similar(positive=['computer'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3 NLTK\n",
    "\n",
    "NLTK的全称为Natural Language Toolkit，\n",
    "\n",
    "安装NLTK：``pip install nltk``\n",
    "\n",
    "参考网站：http://www.nltk.org/\n",
    "https://www.nltk.org/book/\n",
    "\n",
    "在使用NLTK时，如果出现 Resource xxx not found 这样的错误，按照提示用``nltk.download('xxx')``下载所需的资源；如果``nltk.download()``连接失败或超时，可以从https://github.com/nltk/nltk_data 的packages目录下手动下载资源，解压后存放到相应路径。（参考http://www.nltk.org/data.html 的Manual Installation部分）"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 使用NLTK进行POS标注\n",
    "\n",
    "使用NLTK对下面这段文本分词，并进行POS词性标注，打印出POS词性标注的结果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "\n",
    "text = \"Super Bowl 50 was an American football game to determine the champion of the National Football League (NFL) for the 2015 season. The American Football Conference (AFC) champion Denver Broncos defeated the National Football Conference (NFC) champion Carolina Panthers to earn their third Super Bowl title. The game was played on February 7, 2016, at Levi's Stadium in the San Francisco Bay Area at Santa Clara, California. As this was the 50th Super Bowl, the league emphasized the \\\"golden anniversary\\\" with various gold-themed initiatives, as well as temporarily suspending the tradition of naming each Super Bowl game with Roman numerals (under which the game would have been known as \\\"Super Bowl L\\\"), so that the logo could prominently feature the Arabic numerals 50.\"\n",
    "\n",
    "# TODO\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[('Super', 'NNP'),\n",
      " ('Bowl', 'NNP'),\n",
      " ('50', 'CD'),\n",
      " ('was', 'VBD'),\n",
      " ('an', 'DT'),\n",
      " ('American', 'JJ'),\n",
      " ('football', 'NN'),\n",
      " ('game', 'NN'),\n",
      " ('to', 'TO'),\n",
      " ('determine', 'VB')]\n"
     ]
    }
   ],
   "source": [
    "word_cut=nltk.word_tokenize(text)\n",
    "word_pos=nltk.pos_tag(word_cut)\n",
    "from pprint import pprint\n",
    "pprint(word_pos[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 使用NLTK进行命名实体识别\n",
    "\n",
    "常用的命名实体类别有：\n",
    "\n",
    "|NE Type|   Examples|\n",
    "|----|----|\n",
    "|ORGANIZATION|  Georgia-Pacific Corp., WHO|\n",
    "|PERSON|\tEddy Bonte, President Obama|\n",
    "|LOCATION|  Murray River, Mount Everest|\n",
    "|DATE|\tJune, 2008-06-29|\n",
    "|TIME|\ttwo fifty a m, 1:30 p.m.|\n",
    "|MONEY|\t175 million Canadian Dollars, GBP 10.40|\n",
    "|PERCENT|\ttwenty pct, 18.75 %|\n",
    "|FACILITY|\tWashington Monument, Stonehenge|\n",
    "|GPE|\tSouth East Asia, Midlothian|\n",
    "\n",
    "\n",
    "继续处理上面这段文本，用NLTK的``ne_chunk()``方法识别出文本中的命名实体，打印命名实体以及它们所属的类别。\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Entity Name   Entity Type\n",
      "0        Super        PERSON\n",
      "1     American           GPE\n",
      "2     National  ORGANIZATION\n",
      "3          NFL  ORGANIZATION\n",
      "4     American  ORGANIZATION\n",
      "5          AFC  ORGANIZATION\n",
      "6       Denver        PERSON\n",
      "7     National  ORGANIZATION\n",
      "8          NFC  ORGANIZATION\n",
      "9     Carolina        PERSON\n",
      "10       Super        PERSON\n",
      "11        Levi  ORGANIZATION\n",
      "12         San  ORGANIZATION\n",
      "13       Santa      FACILITY\n",
      "14  California           GPE\n",
      "15       Super        PERSON\n",
      "16       Roman        PERSON\n",
      "17       Super        PERSON\n",
      "18      Arabic  ORGANIZATION\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "import pandas as pd \n",
    "chunk_tree=nltk.ne_chunk(word_pos)\n",
    "named_entities = []\n",
    "\n",
    "for tagged_tree in chunk_tree:\n",
    "    if hasattr(tagged_tree, 'label'):\n",
    "        name = tagged_tree[0][0]\n",
    "        word_type = tagged_tree.label()\n",
    "        named_entities.append((name,word_type))\n",
    "\n",
    "entity_frame = pd.DataFrame(named_entities, columns=['Entity Name', 'Entity Type'])\n",
    "print(entity_frame)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 名词短语提取\n",
    "\n",
    "在进行了POS词性标注规则之后，我们可以使用正则表达式自定义语法规则，从文本中提取特定的组成部分，比如名词短语、介词短语、从句等等。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 定义一种用来提取Noun Phrase的语法规则\n",
    "grammar = r\"\"\"\n",
    "  NP: {<DT|PP\\$>?<JJ>*<NN>}   # chunk determiner/possessive, adjectives and noun\n",
    "      {<NNP>+}                # chunk sequences of proper nouns\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "请使用NLTK的RegexpParser从文本中根据上面定义的规则提取名词短语，并打印提取出的名词短语。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Super', 'an', 'game', 'the', 'National', 'NFL', 'season', 'Football', 'AFC', 'champion', 'Denver', 'National', 'NFC', 'champion', 'Carolina', 'Super', 'title', 'The', 'February', 'Levi', 'Stadium', 'San', 'Santa', 'California', 'Super', 'the', 'golden', 'the', 'Super', 'game', 'Roman', 'the', 'Super', 'the', 'Arabic']\n"
     ]
    }
   ],
   "source": [
    "# TODO\n",
    "cp=nltk.RegexpParser(grammar)\n",
    "re_tree=cp.parse(word_pos)\n",
    "named_entities = []\n",
    "\n",
    "for tagged_tree in re_tree:\n",
    "    if hasattr(tagged_tree, 'label'):\n",
    "        name = tagged_tree[0][0]\n",
    "        named_entities.append(name)\n",
    "\n",
    "print(named_entities)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
